{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis - Validation Rule Search System\n",
    "\n",
    "This notebook provides comprehensive performance analysis with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Add project to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from app import boot\n",
    "from rag.search.config import SearchMode\n",
    "from db.manager import DatabaseManager\n",
    "from config import SQLITE_DB_PATH, SQLITE_TABLE_NAME\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure startup time\n",
    "print(\"Initializing system...\")\n",
    "start_time = time.time()\n",
    "app, retriever = boot()\n",
    "startup_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ System initialized in {startup_time:.3f} seconds\")\n",
    "\n",
    "# Initialize database manager\n",
    "db_manager = DatabaseManager(SQLITE_DB_PATH, SQLITE_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_stats():\n",
    "    \"\"\"Get current memory statistics.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return {\n",
    "        'rss_mb': memory_info.rss / 1024 / 1024,\n",
    "        'vms_mb': memory_info.vms / 1024 / 1024,\n",
    "        'percent': process.memory_percent()\n",
    "    }\n",
    "\n",
    "memory_stats = get_memory_stats()\n",
    "\n",
    "# Create memory usage plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Memory breakdown\n",
    "memory_data = pd.DataFrame({\n",
    "    'Type': ['RSS', 'VMS'],\n",
    "    'MB': [memory_stats['rss_mb'], memory_stats['vms_mb']]\n",
    "})\n",
    "sns.barplot(data=memory_data, x='Type', y='MB', ax=ax1, palette='viridis')\n",
    "ax1.set_title('Memory Usage by Type')\n",
    "ax1.set_ylabel('Memory (MB)')\n",
    "\n",
    "# Memory percentage\n",
    "ax2.pie([memory_stats['percent'], 100-memory_stats['percent']], \n",
    "        labels=['Used', 'Available'], \n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#ff9999', '#66b3ff'])\n",
    "ax2.set_title('System Memory Usage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMemory Statistics:\")\n",
    "print(f\"  RSS Memory: {memory_stats['rss_mb']:.1f} MB\")\n",
    "print(f\"  Virtual Memory: {memory_stats['vms_mb']:.1f} MB\")\n",
    "print(f\"  Memory Percent: {memory_stats['percent']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Search Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_search_performance(num_queries=30):\n",
    "    \"\"\"Measure search performance across different modes.\"\"\"\n",
    "    \n",
    "    test_queries = [\n",
    "        \"IBAN\", \"BIC\", \"currency\", \"amount\", \"date\",\n",
    "        \"validate IBAN\", \"check currency\", \"payment amount\",\n",
    "        \"MT103\", \"MT202\", \"pacs.008\", \"pain.001\", \"ISO20022\",\n",
    "        \"BANSTA error\", \"validation failed\", \"invalid format\"\n",
    "    ]\n",
    "    \n",
    "    # Extend queries to reach num_queries\n",
    "    queries = (test_queries * (num_queries // len(test_queries) + 1))[:num_queries]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for mode_name, mode in [\n",
    "        ('Hybrid', SearchMode.HYBRID),\n",
    "        ('Keyword', SearchMode.KEYWORD),\n",
    "        ('Semantic', SearchMode.SEMANTIC),\n",
    "        ('Fuzzy', SearchMode.FUZZY)\n",
    "    ]:\n",
    "        print(f\"Testing {mode_name} mode...\")\n",
    "        latencies = []\n",
    "        \n",
    "        for query in queries:\n",
    "            start = time.perf_counter()\n",
    "            _ = retriever.search_rules(query=query, mode=mode, top_k=10)\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            latencies.append(latency)\n",
    "        \n",
    "        results[mode_name] = latencies\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Measure performance\n",
    "latency_df = measure_search_performance(30)\n",
    "print(\"\\nSearch performance measured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create latency visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Box plot\n",
    "latency_df.boxplot(ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Latency Distribution by Search Mode')\n",
    "axes[0, 0].set_ylabel('Latency (ms)')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Violin plot\n",
    "melted_df = latency_df.melt(var_name='Mode', value_name='Latency')\n",
    "sns.violinplot(data=melted_df, x='Mode', y='Latency', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Latency Distribution (Violin Plot)')\n",
    "axes[0, 1].set_ylabel('Latency (ms)')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Percentiles comparison\n",
    "percentiles = latency_df.describe(percentiles=[.5, .75, .95, .99]).T\n",
    "percentiles[['50%', '95%', '99%']].plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Latency Percentiles Comparison')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].set_xlabel('Search Mode')\n",
    "axes[1, 0].legend(title='Percentile')\n",
    "\n",
    "# Mean and std comparison\n",
    "stats_df = pd.DataFrame({\n",
    "    'Mean': latency_df.mean(),\n",
    "    'Std': latency_df.std()\n",
    "})\n",
    "stats_df.plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Mean Latency and Standard Deviation')\n",
    "axes[1, 1].set_ylabel('Latency (ms)')\n",
    "axes[1, 1].set_xlabel('Search Mode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nLatency Statistics (milliseconds):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Mode':<10} {'Mean':>8} {'Median':>8} {'P95':>8} {'P99':>8} {'Max':>8}\")\n",
    "print(\"-\"*60)\n",
    "for col in latency_df.columns:\n",
    "    print(f\"{col:<10} {latency_df[col].mean():>8.1f} {latency_df[col].median():>8.1f} \"\n",
    "          f\"{latency_df[col].quantile(0.95):>8.1f} {latency_df[col].quantile(0.99):>8.1f} \"\n",
    "          f\"{latency_df[col].max():>8.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Performance Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_filter_impact():\n",
    "    \"\"\"Measure the impact of filters on search performance.\"\"\"\n",
    "    filter_options = retriever.filter_options\n",
    "    results = []\n",
    "    \n",
    "    test_query = \"payment validation\"\n",
    "    \n",
    "    # No filters\n",
    "    latencies_no_filter = []\n",
    "    for _ in range(20):\n",
    "        start = time.perf_counter()\n",
    "        _ = retriever.search_rules(query=test_query, mode=SearchMode.HYBRID, top_k=10)\n",
    "        latencies_no_filter.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    results.append({\n",
    "        'Configuration': 'No Filters',\n",
    "        'Mean': np.mean(latencies_no_filter),\n",
    "        'P95': np.percentile(latencies_no_filter, 95)\n",
    "    })\n",
    "    \n",
    "    # Single filter\n",
    "    if filter_options.get('rule_type'):\n",
    "        latencies_single = []\n",
    "        for _ in range(20):\n",
    "            start = time.perf_counter()\n",
    "            _ = retriever.search_rules(\n",
    "                query=test_query,\n",
    "                rule_type=[filter_options['rule_type'][0]],\n",
    "                mode=SearchMode.HYBRID,\n",
    "                top_k=10\n",
    "            )\n",
    "            latencies_single.append((time.perf_counter() - start) * 1000)\n",
    "        \n",
    "        results.append({\n",
    "            'Configuration': 'Single Filter',\n",
    "            'Mean': np.mean(latencies_single),\n",
    "            'P95': np.percentile(latencies_single, 95)\n",
    "        })\n",
    "    \n",
    "    # Multiple filters\n",
    "    if filter_options.get('rule_type') and filter_options.get('country'):\n",
    "        latencies_multiple = []\n",
    "        for _ in range(20):\n",
    "            start = time.perf_counter()\n",
    "            _ = retriever.search_rules(\n",
    "                query=test_query,\n",
    "                rule_type=[filter_options['rule_type'][0]],\n",
    "                country=[filter_options['country'][0]] if filter_options['country'] else None,\n",
    "                mode=SearchMode.HYBRID,\n",
    "                top_k=10\n",
    "            )\n",
    "            latencies_multiple.append((time.perf_counter() - start) * 1000)\n",
    "        \n",
    "        results.append({\n",
    "            'Configuration': 'Multiple Filters',\n",
    "            'Mean': np.mean(latencies_multiple),\n",
    "            'P95': np.percentile(latencies_multiple, 95)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Measure filter impact\n",
    "filter_df = measure_filter_impact()\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "filter_df.set_index('Configuration')['Mean'].plot(kind='bar', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Mean Latency by Filter Configuration')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "filter_df.set_index('Configuration')['P95'].plot(kind='bar', ax=ax2, color='salmon')\n",
    "ax2.set_title('P95 Latency by Filter Configuration')\n",
    "ax2.set_ylabel('Latency (ms)')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFilter Performance Impact:\")\n",
    "print(filter_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Database and Index Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database read performance\n",
    "db_read_times = []\n",
    "for _ in range(20):\n",
    "    start = time.perf_counter()\n",
    "    rules = db_manager.get_rules()\n",
    "    db_read_times.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "# Index build performance\n",
    "index_build_times = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    retriever._build_indices()\n",
    "    index_build_times.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.hist(db_read_times, bins=15, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(np.mean(db_read_times), color='red', linestyle='--', label=f'Mean: {np.mean(db_read_times):.1f}ms')\n",
    "ax1.axvline(np.percentile(db_read_times, 95), color='orange', linestyle='--', label=f'P95: {np.percentile(db_read_times, 95):.1f}ms')\n",
    "ax1.set_title('Database Read Performance')\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.hist(index_build_times, bins=10, edgecolor='black', alpha=0.7, color='green')\n",
    "ax2.axvline(np.mean(index_build_times), color='red', linestyle='--', label=f'Mean: {np.mean(index_build_times):.1f}ms')\n",
    "ax2.set_title('Index Build Performance')\n",
    "ax2.set_xlabel('Time (ms)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDatabase Performance:\")\n",
    "print(f\"  Read Mean: {np.mean(db_read_times):.2f} ms\")\n",
    "print(f\"  Read P95: {np.percentile(db_read_times, 95):.2f} ms\")\n",
    "print(f\"  Rules Count: {len(rules)}\")\n",
    "print(f\"\\nIndex Build Performance:\")\n",
    "print(f\"  Mean: {np.mean(index_build_times):.2f} ms\")\n",
    "print(f\"  Std: {np.std(index_build_times):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "PERFORMANCE ANALYSIS REPORT\n",
    "{'='*60}\n",
    "\n",
    "1. SYSTEM INITIALIZATION\n",
    "   - Startup Time: {startup_time:.3f} seconds\n",
    "   - Total Rules: {len(rules)}\n",
    "\n",
    "2. MEMORY USAGE\n",
    "   - RSS Memory: {memory_stats['rss_mb']:.1f} MB\n",
    "   - Virtual Memory: {memory_stats['vms_mb']:.1f} MB\n",
    "   - System Memory %: {memory_stats['percent']:.2f}%\n",
    "\n",
    "3. SEARCH LATENCY (ms)\n",
    "   Mode      Mean    Median   P95      P99      Max\n",
    "   {'-'*55}\n",
    "\"\"\"\n",
    "\n",
    "for col in latency_df.columns:\n",
    "    report += f\"   {col:<9} {latency_df[col].mean():>6.1f}  {latency_df[col].median():>6.1f}  \"\n",
    "    report += f\"{latency_df[col].quantile(0.95):>6.1f}  {latency_df[col].quantile(0.99):>6.1f}  \"\n",
    "    report += f\"{latency_df[col].max():>6.1f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "4. FILTER PERFORMANCE IMPACT\n",
    "{filter_df.to_string(index=False, float_format='%.1f')}\n",
    "\n",
    "5. DATABASE PERFORMANCE\n",
    "   - Read Mean: {np.mean(db_read_times):.2f} ms\n",
    "   - Read P95: {np.percentile(db_read_times, 95):.2f} ms\n",
    "\n",
    "6. INDEX BUILD PERFORMANCE\n",
    "   - Mean Time: {np.mean(index_build_times):.2f} ms\n",
    "   - Std Dev: {np.std(index_build_times):.2f} ms\n",
    "\n",
    "{'='*60}\n",
    "Report generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "with open('performance_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "print(\"\\n✓ Report saved to performance_report.txt\")\n",
    "\n",
    "# Save data to JSON\n",
    "performance_data = {\n",
    "    'startup_time': startup_time,\n",
    "    'memory': memory_stats,\n",
    "    'latency_stats': latency_df.describe().to_dict(),\n",
    "    'filter_impact': filter_df.to_dict('records'),\n",
    "    'database': {\n",
    "        'read_mean_ms': np.mean(db_read_times),\n",
    "        'read_p95_ms': np.percentile(db_read_times, 95),\n",
    "        'num_rules': len(rules)\n",
    "    },\n",
    "    'index_build': {\n",
    "        'mean_ms': np.mean(index_build_times),\n",
    "        'std_ms': np.std(index_build_times)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('performance_data.json', 'w') as f:\n",
    "    json.dump(performance_data, f, indent=2, default=str)\n",
    "print(\"✓ Data saved to performance_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and provide recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Check startup time\n",
    "if startup_time > 1.0:\n",
    "    recommendations.append(\"⚠️ Startup time exceeds 1 second. Consider lazy loading or caching indices.\")\n",
    "else:\n",
    "    recommendations.append(\"✓ Startup time is acceptable.\")\n",
    "\n",
    "# Check memory usage\n",
    "if memory_stats['rss_mb'] > 1024:\n",
    "    recommendations.append(\"⚠️ Memory usage exceeds 1GB. Consider optimizing data structures.\")\n",
    "else:\n",
    "    recommendations.append(\"✓ Memory usage is within acceptable limits.\")\n",
    "\n",
    "# Check search latencies\n",
    "if latency_df['Hybrid'].quantile(0.95) > 1000:\n",
    "    recommendations.append(\"⚠️ Hybrid search P95 exceeds 1 second. Consider caching or optimization.\")\n",
    "else:\n",
    "    recommendations.append(\"✓ Hybrid search latency is acceptable.\")\n",
    "\n",
    "if latency_df['Keyword'].mean() > 10:\n",
    "    recommendations.append(\"⚠️ Keyword search is slower than expected. Check BM25 implementation.\")\n",
    "else:\n",
    "    recommendations.append(\"✓ Keyword search performance is excellent.\")\n",
    "\n",
    "# Check filter impact\n",
    "if len(filter_df) > 1:\n",
    "    speedup = filter_df.iloc[0]['Mean'] / filter_df.iloc[-1]['Mean']\n",
    "    if speedup > 1.5:\n",
    "        recommendations.append(f\"✓ Filters provide {speedup:.1f}x speedup.\")\n",
    "    else:\n",
    "        recommendations.append(\"⚠️ Filters don't significantly improve performance.\")\n",
    "\n",
    "print(\"\\nPERFORMANCE RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
