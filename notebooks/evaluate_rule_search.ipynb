{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c919a7c8",
   "metadata": {},
   "source": [
    "\n",
    "# Rule Search Evaluation (Names CSV)\n",
    "\n",
    "This notebook evaluates BM25-only, Semantic-only, Fuzzy-only, and Hybrid search for your Kotlin rule retriever.\n",
    "\n",
    "- **Primary metric:** MRR@5  \n",
    "- **Secondary:** Hit@1/3/5, Coverage  \n",
    "- **Tuning:** Leave-One-Prompt-Out (LOOCV) on hybrid weights (simplex step=0.1)  \n",
    "- **CSV contains rule names** → we map names → IDs robustly (case/space-insensitive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8056d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust these imports to your own package/module paths.\n",
    "from notebooks.rule_retriever_improved import RuleRetriever\n",
    "from your_package.base import SearchConfig, SearchMode   # <-- change 'your_package'\n",
    "from your_package.db.manager import DatabaseManager      # <-- change 'your_package'\n",
    "from your_package.embeddings.embeddings_manager import EmbeddingManager  # <-- change 'your_package'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016ebdd",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load data & initialize retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- User inputs ----\n",
    "CSV_PATH = \"../data/prompts.csv\"  # columns: prompt, expected_rule (RULE NAME)\n",
    "DB_PATH = \"../db/rules.db\"\n",
    "TABLE_NAME = \"rules\"\n",
    "EMBEDDING_MODEL = \"../models/embeddings/UAE-Large-V1\"\n",
    "\n",
    "TOPK_POOL = 20  # per signal for candidate pooling\n",
    "\n",
    "# ---- Load CSV ----\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "assert {'prompt', 'expected_rule'} <= set(df.columns), \"CSV must contain prompt and expected_rule columns\"\n",
    "\n",
    "# ---- Initialize retriever (disable filtering/reranking for eval) ----\n",
    "config = SearchConfig(\n",
    "    semantic_weight=0.6, bm25_weight=0.35, fuzzy_weight=0.05,\n",
    "    min_similarity=0.0, enable_reranking=False\n",
    ")\n",
    "dbm = DatabaseManager(db_path=DB_PATH, table_name=TABLE_NAME); dbm.init_db()\n",
    "emb_mgr = EmbeddingManager(model_name=EMBEDDING_MODEL)\n",
    "retriever = RuleRetriever(embedding_manager=emb_mgr, config=config, db_manager=dbm)\n",
    "\n",
    "print(f\"Loaded {len(df)} prompts and {len(retriever.rules)} rules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914770b8",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Map expected **names** → IDs (robust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c853f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build name -> id map (case/space-insensitive); support both 'rule_name' and 'name' fields.\n",
    "name_to_id = {}\n",
    "for r in retriever.rules:\n",
    "    nm = (r.get('rule_name') or r.get('name') or '').strip().lower()\n",
    "    if nm:\n",
    "        name_to_id[nm] = str(r['rule_id']).strip()\n",
    "\n",
    "def expected_to_id_from_name(x):\n",
    "    return name_to_id.get(str(x).strip().lower(), None)\n",
    "\n",
    "df['expected_id'] = df['expected_rule'].apply(expected_to_id_from_name)\n",
    "n_missing = int(df['expected_id'].isna().sum())\n",
    "print(\"Missing expected_id mapping:\", n_missing)\n",
    "if n_missing:\n",
    "    display(df[df['expected_id'].isna()].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8d3d4",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Candidate pools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def candidate_pool_ids(prompt, k_each=TOPK_POOL):\n",
    "    pool_rules = retriever.candidate_pool(prompt, k_each=k_each)\n",
    "    return [r['rule_id'] for r in pool_rules]\n",
    "\n",
    "pools = {i: candidate_pool_ids(row['prompt']) for i, row in df.iterrows()}\n",
    "\n",
    "def expected_in_pool_stats():\n",
    "    present = 0\n",
    "    issues = []\n",
    "    for i, row in df.iterrows():\n",
    "        exp = row['expected_id']\n",
    "        if not isinstance(exp, str):\n",
    "            issues.append((i, 'MISSING_MAPPING'))\n",
    "            continue\n",
    "        in_pool = exp in pools[i]\n",
    "        if in_pool:\n",
    "            present += 1\n",
    "        else:\n",
    "            issues.append((i, 'NOT_IN_POOL'))\n",
    "    print(f\"Expected present in pool for {present}/{len(df)} prompts.\")\n",
    "    if issues[:5]:\n",
    "        print(\"Examples:\", issues[:5])\n",
    "\n",
    "expected_in_pool_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57179c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_of_expected(expected_id, ranked_ids):\n",
    "    try:\n",
    "        return ranked_ids.index(expected_id) + 1\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def mrr_at_k(ranks, k=5): return float(np.mean([1.0/r if (r is not None and r<=k) else 0.0 for r in ranks]))\n",
    "def hit_at_k(ranks, k=1): return float(np.mean([1.0 if (r is not None and r<=k) else 0.0 for r in ranks]))\n",
    "def coverage(ranks):      return float(np.mean([1.0 if (r is not None) else 0.0 for r in ranks]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c9985",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Ranking helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_per_prompt(score_dict):\n",
    "    if not score_dict: return score_dict\n",
    "    mx = max(score_dict.values())\n",
    "    if mx <= 0: return {k: 0.0 for k in score_dict}\n",
    "    return {k: v/mx for k, v in score_dict.items()}\n",
    "\n",
    "def ranked_ids_by_method(prompt, pool_ids, method, weights=None):\n",
    "    id_to_rule = {r['rule_id']: r for r in retriever.rules}\n",
    "    rules = [id_to_rule[rid] for rid in pool_ids if rid in id_to_rule]\n",
    "\n",
    "    if method == 'bm25':\n",
    "        scores = retriever._bm25_scores(prompt, rules)\n",
    "    elif method == 'semantic':\n",
    "        scores = retriever._semantic_scores(prompt, rules)\n",
    "    elif method == 'fuzzy':\n",
    "        scores = retriever._fuzzy_scores(prompt, rules)\n",
    "    elif method == 'hybrid':\n",
    "        scores = retriever._hybrid_scores(prompt, rules)\n",
    "    elif method == 'hybrid_custom':\n",
    "        sem = normalize_per_prompt(retriever._semantic_scores(prompt, rules))\n",
    "        bm  = normalize_per_prompt(retriever._bm25_scores(prompt, rules))\n",
    "        fz  = normalize_per_prompt(retriever._fuzzy_scores(prompt, rules))\n",
    "        w_sem, w_bm, w_fz = weights\n",
    "        s = w_sem + w_bm + w_fz\n",
    "        if s <= 0: s = 1.0\n",
    "        w_sem, w_bm, w_fz = w_sem/s, w_bm/s, w_fz/s\n",
    "        scores = {rid: w_sem*sem.get(rid,0.0) + w_bm*bm.get(rid,0.0) + w_fz*fz.get(rid,0.0) for rid in pool_ids}\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "    return sorted(pool_ids, key=lambda rid: scores.get(rid, 0.0), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9984c89",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Baselines (no tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_method(method):\n",
    "    ranks = []\n",
    "    for i, row in df.iterrows():\n",
    "        exp = row['expected_id']\n",
    "        if not isinstance(exp, str):  # skip missing mapping\n",
    "            ranks.append(None); continue\n",
    "        ranked = ranked_ids_by_method(row['prompt'], pools[i], method)\n",
    "        ranks.append(rank_of_expected(exp, ranked))\n",
    "    return {\n",
    "        \"MRR@5\": mrr_at_k(ranks, k=5),\n",
    "        \"Hit@1\": hit_at_k(ranks, k=1),\n",
    "        \"Hit@3\": hit_at_k(ranks, k=3),\n",
    "        \"Hit@5\": hit_at_k(ranks, k=5),\n",
    "        \"Coverage\": coverage(ranks),\n",
    "    }\n",
    "\n",
    "baseline_results = {\n",
    "    \"BM25\": evaluate_method(\"bm25\"),\n",
    "    \"Semantic\": evaluate_method(\"semantic\"),\n",
    "    \"Fuzzy\": evaluate_method(\"fuzzy\"),\n",
    "    \"Hybrid (prod)\": evaluate_method(\"hybrid\"),\n",
    "}\n",
    "import pandas as pd\n",
    "pd.DataFrame(baseline_results).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56d9aa",
   "metadata": {},
   "source": [
    "\n",
    "## 7) LOOCV tuning for hybrid weights (simplex step=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build simplex grid\n",
    "grid = []\n",
    "for a in np.round(np.arange(0.0, 1.01, 0.1), 2):\n",
    "    for b in np.round(np.arange(0.0, 1.01 - a, 0.1), 2):\n",
    "        c = round(1.0 - a - b, 2)\n",
    "        if c < -1e-9: continue\n",
    "        grid.append((float(a), float(b), float(c)))\n",
    "\n",
    "indices_all = [i for i in range(len(df)) if isinstance(df.iloc[i]['expected_id'], str)]\n",
    "\n",
    "def eval_weights_on(indices, weights):\n",
    "    ranks = []\n",
    "    for i in indices:\n",
    "        row = df.iloc[i]\n",
    "        ranked = ranked_ids_by_method(row['prompt'], pools[i], 'hybrid_custom', weights=weights)\n",
    "        ranks.append(rank_of_expected(row['expected_id'], ranked))\n",
    "    return mrr_at_k(ranks, k=5)\n",
    "\n",
    "best_weights_per_prompt, ranks_test = [], []\n",
    "for held_out in indices_all:\n",
    "    train_idx = [i for i in indices_all if i != held_out]\n",
    "    best_w, best_score = None, -1.0\n",
    "    for w in grid:\n",
    "        s = eval_weights_on(train_idx, w)\n",
    "        if s > best_score:\n",
    "            best_score, best_w = s, w\n",
    "    best_weights_per_prompt.append(best_w)\n",
    "    row = df.iloc[held_out]\n",
    "    ranked = ranked_ids_by_method(row['prompt'], pools[held_out], 'hybrid_custom', weights=best_w)\n",
    "    ranks_test.append(rank_of_expected(row['expected_id'], ranked))\n",
    "\n",
    "tuned = {\n",
    "    \"MRR@5\": mrr_at_k(ranks_test, k=5),\n",
    "    \"Hit@1\": hit_at_k(ranks_test, k=1),\n",
    "    \"Hit@3\": hit_at_k(ranks_test, k=3),\n",
    "    \"Hit@5\": hit_at_k(ranks_test, k=5),\n",
    "    \"Coverage\": coverage(ranks_test),\n",
    "}\n",
    "import pandas as pd\n",
    "pd.DataFrame([tuned], index=[\"Hybrid (tuned)\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800ad01",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Compare & plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = pd.concat([pd.DataFrame(baseline_results).T, pd.DataFrame([tuned], index=[\"Hybrid (tuned)\"])])\n",
    "display(all_results)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(all_results.index, all_results[\"MRR@5\"])\n",
    "plt.title(\"MRR@5 by Method\")\n",
    "plt.ylabel(\"MRR@5\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d66622",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Ablation & sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb2d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_arr = np.array(best_weights_per_prompt)\n",
    "w_med = tuple(np.median(w_arr, axis=0)) if len(w_arr) else (0.6, 0.35, 0.05)\n",
    "s = sum(w_med) or 1.0\n",
    "w_sem, w_bm, w_fz = (w_med[0]/s, w_med[1]/s, w_med[2]/s)\n",
    "print(\"Median tuned weights:\", (w_sem, w_bm, w_fz))\n",
    "\n",
    "def evaluate_custom_weights(weights):\n",
    "    ranks = []\n",
    "    for i in indices_all:\n",
    "        row = df.iloc[i]\n",
    "        ranked = ranked_ids_by_method(row['prompt'], pools[i], 'hybrid_custom', weights=weights)\n",
    "        ranks.append(rank_of_expected(row['expected_id'], ranked))\n",
    "    return {\n",
    "        \"MRR@5\": mrr_at_k(ranks, k=5),\n",
    "        \"Hit@1\": hit_at_k(ranks, k=1),\n",
    "        \"Hit@3\": hit_at_k(ranks, k=3),\n",
    "        \"Hit@5\": hit_at_k(ranks, k=5),\n",
    "        \"Coverage\": coverage(ranks),\n",
    "    }\n",
    "\n",
    "# Ablations\n",
    "abl = {}\n",
    "ws = (0.0, w_bm, w_fz); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); abl[\"No Semantic\"] = evaluate_custom_weights(ws)\n",
    "ws = (w_sem, 0.0, w_fz); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); abl[\"No BM25\"]   = evaluate_custom_weights(ws)\n",
    "ws = (w_sem, w_bm, 0.0); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); abl[\"No Fuzzy\"]  = evaluate_custom_weights(ws)\n",
    "display(pd.DataFrame(abl).T)\n",
    "\n",
    "# Sensitivity\n",
    "def clip01(x): return max(0.0, min(1.0, float(x)))\n",
    "def sens(center_w, deltas=[-0.2,-0.1,0.0,0.1,0.2]):\n",
    "    w_sem, w_bm, w_fz = center_w\n",
    "    rows = []\n",
    "    for d in deltas:\n",
    "        ws = (clip01(w_sem+d), w_bm, w_fz); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); rows.append((\"sem\", d, evaluate_custom_weights(ws)[\"MRR@5\"]))\n",
    "        ws = (w_sem, clip01(w_bm+d), w_fz); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); rows.append((\"bm25\", d, evaluate_custom_weights(ws)[\"MRR@5\"]))\n",
    "        ws = (w_sem, w_bm, clip01(w_fz+d)); s = sum(ws) or 1.0; ws = tuple(w/s for w in ws); rows.append((\"fuzzy\", d, evaluate_custom_weights(ws)[\"MRR@5\"]))\n",
    "    return pd.DataFrame(rows, columns=[\"weight\",\"delta\",\"MRR@5\"])\n",
    "\n",
    "sens_df = sens((w_sem, w_bm, w_fz)); display(sens_df)\n",
    "\n",
    "for wname in [\"sem\",\"bm25\",\"fuzzy\"]:\n",
    "    sub = sens_df[sens_df[\"weight\"]==wname]\n",
    "    plt.figure()\n",
    "    plt.plot(sub[\"delta\"], sub[\"MRR@5\"], marker='o')\n",
    "    plt.title(f\"Sensitivity: {wname} vs MRR@5\")\n",
    "    plt.xlabel(\"Delta\")\n",
    "    plt.ylabel(\"MRR@5\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
